{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def startLog(modelName):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"Start running {modelName} model\")\n",
    "    \n",
    "def finishLog(modelName, val_mse, fileName):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"MSE of Validation Set: {val_mse}\")\n",
    "    logger.info(f\"Add file name:{fileName}\")\n",
    "    logger.info(f\"Finish running {modelName} model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def controlModel(X_train, X_val, Y_train, Y_val, random_key, model, modelName):\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_val)\n",
    "    \n",
    "    val_mse = mean_squared_error(Y_val, Y_pred)\n",
    "    \n",
    "    fileName = f\"{modelName}_{int(val_mse)}.csv\"\n",
    "    \n",
    "    return Y_pred, val_mse, fileName, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def runXGBoostRegressor(X_train, X_val, Y_train, Y_val, random_key, modelName, parameters={}):\n",
    "    if parameters:\n",
    "        n_estimators = parameters[\"n_estimators\"]\n",
    "        max_depth = parameters[\"max_depth\"]\n",
    "        learning_rate = parameters[\"learning_rate\"]\n",
    "    else:\n",
    "        n_estimators = 400\n",
    "        max_depth = 6\n",
    "        learning_rate = 0.1\n",
    "        \n",
    "    startLog(modelName)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(f\"parameters: n_estimators:{n_estimators}, max_depth:{max_depth}, learning_rate:{learning_rate}\")\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:squarederror',  # 目標函數，使用平方誤差\n",
    "        n_estimators=n_estimators,              # 樹的數量\n",
    "        max_depth=max_depth,                   # 樹的最大深度\n",
    "        learning_rate=learning_rate,             # 學習率\n",
    "        subsample=0.8,                 # 子採樣比率\n",
    "        colsample_bytree=0.8,          # 每棵樹的列採樣比率\n",
    "        random_state=random_key,       # 控制隨機性\n",
    "        device = \"cuda:0\"\n",
    "    )\n",
    "\n",
    "    Y_pred, val_mse, fileName, model = controlModel(X_train, X_val, Y_train, Y_val, random_key, model, modelName)\n",
    "\n",
    "    print('訓練集: ',model.score(X_train,Y_train))\n",
    "    print('測試集: ',model.score(X_val,Y_val))\n",
    "    finishLog(modelName, val_mse, fileName)\n",
    "\n",
    "    return model, Y_pred, fileName, val_mse\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "def runKNN(X_train, X_val, Y_train, Y_val, random_key, modelName):\n",
    "    startLog(modelName)\n",
    "\n",
    "    model = KNeighborsRegressor(\n",
    "        n_neighbors=95,             # 鄰居數量(1~無窮大)\n",
    "        weights='uniform',          # 鄰居加權方式(\"uniform\" 或 \"distance\")\n",
    "        algorithm='auto',           # 搜索算法(\"auto\", \"ball_tree\", \"kd_tree\", \"brute\")\n",
    "        leaf_size=30,               # 樹的葉節點大小(1~無窮大)\n",
    "        p=2                         # 距離度量方式(1=曼哈頓距離, 2=歐幾里得距離)\n",
    "    )\n",
    "    \n",
    "    Y_pred, val_mse, fileName, model = controlModel(X_train, X_val, Y_train, Y_val, random_key, model, modelName)\n",
    "    \n",
    "    print('訓練集: ',model.score(X_train,Y_train))\n",
    "    print('測試集: ',model.score(X_val,Y_val))\n",
    "    finishLog(modelName, val_mse, fileName)\n",
    "\n",
    "    return model, Y_pred, fileName, val_mse\n",
    "\n",
    "# In[] Decision Tree Regression model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def runDecisionTreeRegressor(X_train, X_val, Y_train, Y_val, random_key, modelName):\n",
    "    startLog(modelName)\n",
    "\n",
    "    model = DecisionTreeRegressor(\n",
    "        criterion='squared_error',   # 測量分裂品質的標準(\"squared_error\" 或 \"absolute_error\")\n",
    "        splitter='best',             # 選擇分裂點的方法(\"best\" 或 \"random\")\n",
    "        max_depth=14,                # 樹的最大深度(1~無窮大)\n",
    "        min_samples_split=300,         # 分裂內部節點所需的最小樣本數(2~無窮大)\n",
    "        min_samples_leaf=20,          # 葉子節點所需的最小樣本數(1~無窮大)\n",
    "        max_features=None,           # 用於分裂的特徵數量(None 或 1~特徵數量)\n",
    "        random_state=random_key      # 控制隨機性(整數)\n",
    "    )\n",
    "    \n",
    "    Y_pred, val_mse, fileName, model = controlModel(X_train, X_val, Y_train, Y_val, random_key, model, modelName)\n",
    "    \n",
    "    print('訓練集: ',model.score(X_train,Y_train))\n",
    "    print('測試集: ',model.score(X_val,Y_val))\n",
    "    finishLog(modelName, val_mse, fileName)\n",
    "\n",
    "    return model, Y_pred, fileName, val_mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "def decomposition(dataset, x_columns, y_columns=None):\n",
    "    X = dataset[x_columns]\n",
    "    Y = dataset[y_columns] if y_columns else None\n",
    "    return (X, Y) if Y is not None else X\n",
    "\n",
    "def split_test_data(dataset, isMean=True):\n",
    "    dataset['y'] = dataset['序號'].astype(str).str[0:4].astype(int).squeeze()\n",
    "    dataset['mo'] = dataset['序號'].astype(str).str[4:6].astype(int).squeeze()\n",
    "    dataset['d'] = dataset['序號'].astype(str).str[6:8].astype(int).squeeze()\n",
    "    dataset['h'] = dataset['序號'].astype(str).str[8:10].astype(int).squeeze()\n",
    "    dataset['min'] = dataset['序號'].astype(str).str[10:12].astype(int).squeeze()\n",
    "    dataset['LocationCode'] = dataset['序號'].astype(str).str[12:14].astype(int).squeeze()\n",
    "    \n",
    "    sequence = dataset[\"序號\"]\n",
    "    \n",
    "    if isMean:\n",
    "        datasets = []\n",
    "        for i in range(10):\n",
    "            dataset_cp = dataset.copy()\n",
    "            dataset_cp['min'] = dataset['min'] + i\n",
    "            datasets.append(dataset_cp)\n",
    "        dataset = pd.concat(datasets, ignore_index=True)\n",
    "    return dataset, sequence\n",
    "\n",
    "def split_date_time(dataset) ->  pd.DataFrame:\n",
    "    dataset['DateTime'] = pd.to_datetime(dataset['DateTime'])\n",
    "    \n",
    "    dataset['y'] = dataset['DateTime'].dt.year.squeeze()\n",
    "    dataset['mo'] = dataset['DateTime'].dt.month.squeeze()\n",
    "    dataset['d'] = dataset['DateTime'].dt.day.squeeze()\n",
    "    dataset['h'] = dataset['DateTime'].dt.hour.squeeze()\n",
    "    dataset['min'] = dataset['DateTime'].dt.minute.squeeze()\n",
    "    \n",
    "    dataset = dataset.drop(columns=['DateTime'])\n",
    "    \n",
    "    return  dataset\n",
    "\n",
    "\n",
    "def concat_dataset(directory:str) -> pd.DataFrame:\n",
    "    datasets = pd.DataFrame()\n",
    "    for counter, file_name in enumerate(os.listdir(directory)):\n",
    "        dataset = pd.read_csv(os.path.join(directory,file_name) )\n",
    "        datasets = pd.concat([datasets,dataset],ignore_index=True)\n",
    "    return datasets\n",
    "\n",
    "def preprocessing(train_dirs, X_cols, Y_cols, test_set_size=0.2, random_key=0):\n",
    "    dataset = concat_dataset(train_dirs)\n",
    "    dataset = split_date_time(dataset)\n",
    "    X, Y = decomposition(dataset, x_columns=X_cols, y_columns=Y_cols)\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=test_set_size, random_state=random_key)\n",
    "    \n",
    "    return X_train, X_val, Y_train, Y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting split testing set\n",
      "INFO:__main__:Starting indirect prediction with 4 layers\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, filemode='w')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "train_dirs = \"dataset\"\n",
    "test_dir = \"submit/test_data.csv\"\n",
    "result_dir = \"submit/indirect4_\"\n",
    "\n",
    "basic_columns = ['y', 'mo', 'd', 'h', 'min', 'LocationCode']\n",
    "feature_columns_2 = ['Temperature(°C)', 'Humidity(%)']\n",
    "feature_columns_3 = ['Sunlight(Lux)']\n",
    "target_columns = ['Power(mW)']\n",
    "\n",
    "val_set_size = 0.0000001\n",
    "\n",
    "model_names = [\"XGBoost\", \"XGBoost\", \"XGBoost\", \"XGBoost\"]\n",
    "\n",
    "isMean = True\n",
    "\n",
    "logger.info(\"Starting split testing set\")\n",
    "if isMean:\n",
    "    result_dir+=\"Average\"\n",
    "else:\n",
    "    result_dir+=\"noAverage\"\n",
    "    \n",
    "test_dataset = pd.read_csv(test_dir)\n",
    "test_dataset= test_dataset.drop([\"答案\"],axis=1)\n",
    "test_dataset, sequence = split_test_data(test_dataset, isMean)\n",
    "\n",
    "logger.info(\"Starting indirect prediction with 4 layers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_key = 48763 \n",
    "# logger.info(\"Starting preprocessing in layer 1\")\n",
    "# feature_colum = basic_columns \n",
    "# X_train, X_val, Y_train, Y_val = preprocessing(train_dirs, feature_colum, feature_columns_1, val_set_size, random_key)\n",
    "# logger.info(\"Starting model training in layer 1\")\n",
    "# parameters = { \"n_estimators\" : 600 , \"max_depth\" : 8 , \"learning_rate\" : 0.1}\n",
    "# random_key = 48763 \n",
    "# model, Y_pred, filename, val_mse = runXGBoostRegressor(X_train, X_val, Y_train, Y_val, random_key, model_names[0],parameters)\n",
    "# result_dir = result_dir + filename\n",
    "# logger.info(\"Starting prediction on test set in layer 1\")\n",
    "# X_pred = test_dataset[feature_colum]\n",
    "# Y_test = pd.DataFrame(model.predict(X_pred), columns=feature_columns_1)\n",
    "# test_dataset = pd.concat([test_dataset, Y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators:1800, max_depth:8, learning_rate:0.1 33 0.62 0.43\n",
    "0.62 0.83\n",
    "\n",
    "0.3\n",
    "訓練集:  0.6447689533233643\n",
    "測試集:  0.8108588457107544\n",
    "0.4\n",
    "訓練集:  0.6466488838195801\n",
    "測試集:  0.8208843469619751\n",
    "0.05\n",
    "訓練集:  0.5954268574714661\n",
    "測試集:  0.8237078189849854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4334\n",
    "n_estimators:1800, max_depth:8, learning_rate:0.1 0\n",
    "0\n",
    "n_estimators:1800, max_depth:8,  learning_rate:0.1  0.2187 0.62 0.96\n",
    "n_estimators:1800, max_depth:10, learning_rate:0.08 0.2261 0.66 0.97\n",
    "n_estimators:1800, max_depth:10, learning_rate:0.09 0.3222 0.66 0.97\n",
    "n_estimators:1800, max_depth:10, learning_rate:0.1  0.1059 0.66 0.97\n",
    "n_estimators:1800, max_depth:10, learning_rate:0.2  0.1248 0.67 0.97\n",
    "n_estimators:1600, max_depth:12, learning_rate:0.1  0.5747 0.69 0.95\n",
    "n_estimators:1800, max_depth:12, learning_rate:0.1  0.6017 0.69 0.95\n",
    "n_estimators:1800, max_depth:12, learning_rate:0.2  0.8980 0.70 0.94\n",
    "n_estimators:1800, max_depth:12, learning_rate:0.08 0.4094 0.69 0.96\n",
    "n_estimators:1800, max_depth:14, learning_rate:0.08 0.8275 0.70 0.93\n",
    "48763 0\n",
    "n_estimators:1800, max_depth:8,  learning_rate:0.1  0.0781 0.62 0.89  \n",
    "48763 48763"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13 5 2.02 1.97 1.88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting preprocessing in layer 2\n",
      "INFO:__main__:Starting model training in layer 2\n",
      "INFO:__main__:Start running XGBoost model\n",
      "INFO:__main__:parameters: n_estimators:800, max_depth:12, learning_rate:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集:  0.991381824016571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "INFO:__main__:MSE of Validation Set: 4.08333615829359\n",
      "INFO:__main__:Add file name:XGBoost_4.csv\n",
      "INFO:__main__:Finish running XGBoost model\n",
      "INFO:__main__:Starting prediction on test set in layer 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試集:  nan\n"
     ]
    }
   ],
   "source": [
    "random_key = 48763\n",
    "logger.info(\"Starting preprocessing in layer 2\")\n",
    "feature_colum = basic_columns\n",
    "X_train, X_val, Y_train, Y_val = preprocessing(train_dirs, feature_colum, feature_columns_2, val_set_size, random_key)\n",
    "logger.info(\"Starting model training in layer 2\")\n",
    "parameters = { \"n_estimators\" : 800 , \"max_depth\" : 12, \"learning_rate\" : 0.3}\n",
    "random_key = 48763\n",
    "model, Y_pred, filename, val_mse = runXGBoostRegressor(X_train, X_val, Y_train, Y_val, random_key, model_names[1], parameters)\n",
    "result_dir = result_dir + \"_\" + filename\n",
    "logger.info(\"Starting prediction on test set in layer 2\")\n",
    "X_pred = test_dataset[feature_colum]\n",
    "Y_test = pd.DataFrame(model.predict(X_pred), columns=feature_columns_2)\n",
    "test_dataset = pd.concat([test_dataset, Y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48763 0\n",
    "n_estimators:800, max_depth:8,  learning_rate:0.1   78.9048 0.9475\n",
    "n_estimators:800, max_depth:10, learning_rate:0.1   39.3299 0.9701\n",
    "n_estimators:800, max_depth:10, learning_rate:0.15  25.6399 0.9751\n",
    "n_estimators:800, max_depth:10, learning_rate:0.2   18.8055 0.9790\n",
    "n_estimators:800, max_depth:10, learning_rate:0.25  10.8917 0.9811\n",
    "n_estimators:800, max_depth:10, learning_rate:0.275 12.8694 0.9823\n",
    "n_estimators:800, max_depth:10, learning_rate:0.275 15      0.982\n",
    "48763 48763\n",
    "n_estimators:800, max_depth:12, learning_rate:0.25  4.8156  0.9909\n",
    "n_estimators:800, max_depth:12, learning_rate:0.3   4.0833  0.9913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators:600, max_depth:13, learning_rate:0.3 1.48\n",
    "n_estimators:600, max_depth:13, learning_rate:0.2 1.34\n",
    "n_estimators:600, max_depth:15, learning_rate:0.2 1.31\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting preprocessing in layer 3\n",
      "INFO:__main__:Starting model training in layer 3\n",
      "INFO:__main__:Start running XGBoost model\n",
      "INFO:__main__:parameters: n_estimators:800, max_depth:12, learning_rate:0.3\n",
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "INFO:__main__:MSE of Validation Set: 136360.04440796515\n",
      "INFO:__main__:Add file name:XGBoost_136360.csv\n",
      "INFO:__main__:Finish running XGBoost model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集:  0.9986770749092102\n",
      "測試集:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting prediction on test set in layer 3\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting preprocessing in layer 3\")\n",
    "random_key = 48763\n",
    "feature_colum = basic_columns  + feature_columns_2\n",
    "X_train, X_val, Y_train, Y_val = preprocessing(train_dirs, feature_colum, feature_columns_3, val_set_size, random_key)\n",
    "logger.info(\"Starting model training in layer 3\")\n",
    "parameters = { \"n_estimators\" : 800 , \"max_depth\" : 12 , \"learning_rate\" : 0.3}\n",
    "random_key = 48763\n",
    "model, Y_pred, filename, val_mse = runXGBoostRegressor(X_train, X_val, Y_train, Y_val, random_key, model_names[2], parameters)\n",
    "result_dir = result_dir[:-4] + \"_\" + filename\n",
    "logger.info(\"Starting prediction on test set in layer 3\")\n",
    "X_pred = test_dataset[feature_colum]\n",
    "Y_test = pd.DataFrame(model.predict(X_pred), columns=feature_columns_3)\n",
    "test_dataset = pd.concat([test_dataset, Y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators:800, max_depth:8,  learning_rate:0.1 848435 0.9464\n",
    "n_estimators:800, max_depth:8,  learning_rate:0.2 615394 0.9589\n",
    "n_estimators:800, max_depth:8,  learning_rate:0.3 595565 0.9644\n",
    "n_estimators:800, max_depth:10, learning_rate:0.3 540262 0.9892\n",
    "n_estimators:800, max_depth:12, learning_rate:0.3 136360 0.9986"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators:1600, max_depth:11, learning_rate:0.15 32160030\n",
    "n_estimators:1800, max_depth:11, learning_rate:0.15 32062256\n",
    "n_estimators:1800, max_depth:11, learning_rate:0.1  32007405\n",
    "n_estimators:1800, max_depth:11, learning_rate:0.2  34593299\n",
    "n_estimators:1800, max_depth:12, learning_rate:0.1  30477769\n",
    "n_estimators:1800, max_depth:13, learning_rate:0.05 30100012\n",
    "n_estimators:1800, max_depth:14, learning_rate:0.02 29767614\n",
    "n_estimators:1800, max_depth:14, learning_rate:0.05 29341979\n",
    "n_estimators:1800, max_depth:14, learning_rate:0.08 29912468\n",
    "n_estimators:1800, max_depth:14, learning_rate:0.1  30660590\n",
    "n_estimators:1800, max_depth:14, learning_rate:0.2  32822677\n",
    "n_estimators:1400, max_depth:12, learning_rate:0.15 31574809"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting preprocessing in layer 4\n",
      "INFO:__main__:Starting model training in layer 4\n",
      "INFO:__main__:Start running XGBoost model\n",
      "INFO:__main__:parameters: n_estimators:800, max_depth:10, learning_rate:0.25\n",
      "c:\\Users\\User\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "INFO:__main__:MSE of Validation Set: 1.2827553299903869\n",
      "INFO:__main__:Add file name:XGBoost_1.csv\n",
      "INFO:__main__:Finish running XGBoost model\n",
      "INFO:__main__:Starting prediction on test set in layer 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集:  0.9993041753768921\n",
      "測試集:  nan\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting preprocessing in layer 4\")\n",
    "random_key = 48763\n",
    "feature_colum = basic_columns  + feature_columns_2 + feature_columns_3\n",
    "X_train, X_val, Y_train, Y_val = preprocessing(train_dirs, feature_colum, target_columns, val_set_size, random_key)\n",
    "logger.info(\"Starting model training in layer 4\")\n",
    "parameters = { \"n_estimators\" : 800 , \"max_depth\" : 10 , \"learning_rate\" : 0.25}\n",
    "random_key = 48763\n",
    "model, Y_pred, filename, val_mse = runXGBoostRegressor(X_train, X_val, Y_train, Y_val, random_key, model_names[3], parameters)\n",
    "result_dir = result_dir[:-4] + \"_\" + filename\n",
    "logger.info(\"Starting prediction on test set in layer 4\")\n",
    "X_pred = test_dataset[feature_colum]\n",
    "Y_test = pd.DataFrame(model.predict(X_pred), columns=[\"答案\"])\n",
    "Y_test = Y_test.round(2)\n",
    "test_dataset = pd.concat([test_dataset, Y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators:800, max_depth:8,  learning_rate:0.1  2.6847 0.9928\n",
    "n_estimators:800, max_depth:8,  learning_rate:0.2  1.8866 0.9953\n",
    "n_estimators:800, max_depth:8,  learning_rate:0.3  322.81 0.9963\n",
    "n_estimators:800, max_depth:8,  learning_rate:0.25 0.2991 0.9959\n",
    "n_estimators:800, max_depth:10, learning_rate:0.25 1.2827 0.9993"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators:1000, max_depth:6, learning_rate:0.1  3548\n",
    "n_estimators:1800, max_depth:6, learning_rate:0.1  2980\n",
    "n_estimators:1800, max_depth:6, learning_rate:0.1  2915\n",
    "n_estimators:1800, max_depth:6, learning_rate:0.12 2820\n",
    "n_estimators:1800, max_depth:6, learning_rate:0.15 2769\n",
    "n_estimators:1800, max_depth:6, learning_rate:0.2  2545\n",
    "n_estimators:1800, max_depth:6, learning_rate:0.3  2553\n",
    "n_estimators:1800, max_depth:6, learning_rate:0.4  2425\n",
    "n_estimators:1800, max_depth:6, learning_rate:0.5  2541\n",
    "n_estimators:1800, max_depth:8, learning_rate:0.3  2010\n",
    "n_estimators:1800, max_depth:10, learning_rate:0.3 1767\n",
    "n_estimators:1800, max_depth:12, learning_rate:0.3 2073\n",
    "n_estimators:1800, max_depth:12, learning_rate:0.1 1611\n",
    "n_estimators:1800, max_depth:14, learning_rate:0.02 1588"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isMean:\n",
    "    combine_test_dataset = test_dataset.groupby(['序號','y', 'mo', 'd', 'h', 'LocationCode'], as_index=False).mean()\n",
    "    combine_test_dataset['序號'] = combine_test_dataset['序號'].astype('longlong')\n",
    "    combine_test_dataset['序號'] = pd.Categorical(combine_test_dataset['序號'], categories=sequence, ordered=True)\n",
    "    combine_test_dataset = combine_test_dataset.sort_values(by='序號', ascending=True).reset_index(drop=True)\n",
    "    combine_test_dataset['答案'] = combine_test_dataset['答案'].apply(lambda x: max(x, 0))\n",
    "    combine_test_dataset[\"答案\"] = combine_test_dataset[\"答案\"].round(2)\n",
    "    result = pd.concat([combine_test_dataset[\"序號\"], combine_test_dataset[\"答案\"]], axis=1)\n",
    "else:\n",
    "    result = pd.concat([test_dataset[\"序號\"], test_dataset[\"答案\"]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saving new results to submit/indirect4_Average_XGBoost_4_XGBoost_136360_XGBoost_1.csv\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Saving new results to {result_dir}\")\n",
    "result.to_csv(result_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "數量\n",
      "3745589.49\n",
      "3832750.0\n",
      "3695934.62\n",
      "3891905.7600000002\n",
      "3597195.47\n",
      "2911148.850438615\n",
      "2930931.18\n",
      "3645909.96\n",
      "比較\n",
      "1257988.27\n",
      "1276564.7200000002\n",
      "1252931.12\n",
      "1454959.06\n",
      "1373820.15\n",
      "1429291.489713179\n",
      "1528358.18\n"
     ]
    }
   ],
   "source": [
    "comparedir1 = \"submit/713968.02.csv\"\n",
    "comparedir2 = \"submit/732292.96.csv\"\n",
    "comparedir3 = \"submit/735867.9.csv\"\n",
    "comparedir4 = \"submit/802591.63.csv\"\n",
    "comparedir5 = \"submit/833177.17.csv\"\n",
    "comparedir6 = \"submit/895705.41.csv\"\n",
    "comparedir7 = \"submit/919745.63.csv\"\n",
    "comparedir8 = \"submit/963002.63.csv\"\n",
    "testdir = result_dir\n",
    "\n",
    "compare_set1 = pd.read_csv(comparedir1)\n",
    "compare_set2 = pd.read_csv(comparedir2)\n",
    "compare_set3 = pd.read_csv(comparedir3)\n",
    "compare_set4 = pd.read_csv(comparedir4)\n",
    "compare_set5 = pd.read_csv(comparedir5)\n",
    "compare_set6 = pd.read_csv(comparedir6)\n",
    "compare_set7 = pd.read_csv(comparedir7)\n",
    "test = pd.read_csv(testdir)\n",
    "\n",
    "print(\"數量\")\n",
    "print(compare_set1[\"答案\"].sum())\n",
    "print(compare_set2[\"答案\"].sum())\n",
    "print(compare_set3[\"答案\"].sum())\n",
    "print(compare_set4[\"答案\"].sum())\n",
    "print(compare_set5[\"答案\"].sum())\n",
    "print(compare_set6[\"答案\"].sum())\n",
    "print(compare_set7[\"答案\"].sum())\n",
    "print(test[\"答案\"].sum())\n",
    "\n",
    "print(\"比較\")\n",
    "print((test[\"答案\"]-compare_set1[\"答案\"]).abs().sum())\n",
    "print((test[\"答案\"]-compare_set2[\"答案\"]).abs().sum())\n",
    "print((test[\"答案\"]-compare_set3[\"答案\"]).abs().sum())\n",
    "print((test[\"答案\"]-compare_set4[\"答案\"]).abs().sum())\n",
    "print((test[\"答案\"]-compare_set5[\"答案\"]).abs().sum())\n",
    "print((test[\"答案\"]-compare_set6[\"答案\"]).abs().sum())\n",
    "print((test[\"答案\"]-compare_set7[\"答案\"]).abs().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indirect4_noAverage_XGBoost_0_XGBoost_1_XGBoost_4807805_XGBoost_1853.csv\n",
    "random_key = 48763\n",
    "parameters = { \"n_estimators\" : 700 , \"max_depth\" : 6 , \"learning_rate\" : 0.1}\n",
    "random_key = 48763\n",
    "\n",
    "random_key = 48763\n",
    "parameters = { \"n_estimators\" : 600 , \"max_depth\" : 15 , \"learning_rate\" : 0.2}\n",
    "random_key = 0\n",
    "\n",
    "random_key = 48763\n",
    "parameters = { \"n_estimators\" : 1800 , \"max_depth\" : 14 , \"learning_rate\" : 0.05}\n",
    "random_key = 48763\n",
    "\n",
    "random_key = 48763\n",
    "parameters = { \"n_estimators\" : 1800 , \"max_depth\" : 14 , \"learning_rate\" : 0.05}\n",
    "random_key = 48763"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
